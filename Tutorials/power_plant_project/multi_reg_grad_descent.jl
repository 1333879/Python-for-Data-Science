# Import needed libraries
using Plots
using XLSX
using DataFrames
using Statistics


# Read the relevant excel workbook
df = DataFrames.DataFrame(XLSX.readtable("Folds5x2_pp.xlsx", "Sheet5")...)

# Split into design matrix and target vector
X = df[:, 1:4]
design_matrix = convert(Matrix, X)

y = df[:, 5]  # Target vector

# Split into training and test
train_size = 0.80
data_size = size(design_matrix)[1]

train_index = trunc(Int, train_size * data_size)

# Split using the desired train size
X_train = design_matrix[1:train_index, :]
X_test = design_matrix[train_index+1:end, :]

y_train = y[1:train_index]
y_test = y[train_index+1:end]


"""
    scale_features(X)

This function attempts to normalise the design matrix (X) user pass.
The input data X is standardised and return along with other learned metrics.

A tuple with 3 elements is returned representing (Standardised data, mean, std deviation).

"""
function scale_features(X)

    μ = mean(X, dims=1)
    σ = std(X, dims=1)

    X_norm = (X .- μ) ./ σ

    return (X_norm, μ, σ)
end


"""
    transform_features(X, μ, σ)

This functions uses the mean and standard deviation values users pass to
normalise a new design matrix.
"""
function transform_features(X, μ, σ)
    X_norm = (X .- μ) ./ σ
    return X_norm
end

# Scale training features and get artificats for future use
X_train_scaled, μ, σ = scale_features(X_train)

# Transform the testing features by using the learned artifacts
X_test_scaled = transform_features(X_test, μ, σ)


"""
    mean_squared_cost(X, y, θ)

This function computes the batch cost based on the values of the design matrix (X),
target vector (y), and the weights (θ) passed to it.
"""
function mean_squared_cost(X, y, θ)
    # Sample size
    m = size(X)[1]

    # Vectorised Prediction loss
    preds = X * θ
    loss = preds - y

    # Half mean squared loss
    cost =  (1/(2m)) * (loss' * loss)

    return cost
end


"""
    lin_reg_grad_descent(X, y, α, fit_intercept=true, n_iter=1000)

This function uses gradient descent algorithm to find the best weights (θ)
that minimises the mean squared loss between the predictions that the model
generates and the target vector (y).

A tuple of 1D vectors representing the weights (θ)
and a history of loss at each iteration (𝐉) is returned.
"""
function lin_reg_grad_descent(X, y, α, fit_intercept=true, n_iter=2000)
    # Initialize some useful values
    m = length(y) # number of training examples

    if fit_intercept
        # Add a constant of 1s if fit_intercept is specified
        constant = ones(m, 1)
        X = hcat(constant, X)
    else
        X # Assume user added constants
    end

    # Use the number of features to initialise the theta θ vector
    n = size(X)[2]
    θ = zeros(n)

    # Initialise the cost vector based on the number of iterations
    𝐉 = zeros(n_iter)

    for iter in range(1, stop=n_iter)
        pred = X * θ

        # Calcaluate the cost for each iter
        𝐉[iter] = mean_squared_cost(X, y, θ)

        # Update the theta θ at each iter
        θ = θ - ((α/m) * X') * (pred - y);
    end
    return (θ, 𝐉)
end


θ, 𝐉 = lin_reg_grad_descent(X_train_scaled, y_train, 0.05, true, 3000)

plot(𝐉,
     label="Cost per iter",
     ylabel="Cost",
     xlabel="Number of Iteration",
     title="Cost Per Iteration")

savefig("cost_plot")

"""
    predict(X, θ, fit_intercept=true)

This function uses the learned weights (θ) to make predictions based on the
design matrix passed as (X).

The 1D vector representing these predictions is finally returned.
"""
function predict(X, θ, fit_intercept=true)
    m = size(X)[1]

    if fit_intercept
        constant = ones(m)
        X = hcat(constant, X)
    else
        X
    end

    predictions = X * θ

    return predictions
end


# Make predictions for both training and testing datasets
train_preds = predict(X_train_scaled, θ)
test_preds = predict(X_test_scaled, θ)



"""
    rmse_score(y_true, y_pred)

This function calculates the RMSE based on the values of ground truth (y_true)
and the predicted values generated by a model (y_pred)
"""
function rmse_score(y_true, y_pred)

    errors = y_pred - y_true
    errors² = errors .^ 2
    mse = mean(errors²)
    rmse = sqrt(mse)

    return rmse

end


println("RMSE for Training Set: ", rmse_score(y_train, train_preds))
println("RMSE for Testing Set: ", rmse_score(y_test, test_preds))



"""
    r_squared_score(y_pred, y_true)

This function returns the R squared value based on the predictions (y_pred)
and the ground truth values (y_true)passed to it.
"""
function r_squared_score(y_pred, y_true)
    # Just a convinient way of using notations
    ∑ = sum
    μ = mean

    # Compute sum of explained variance (SST) and sum of squares of residuals
    sst = ∑(((y_true .- μ(y_true)) .^ 2))
    ssr = ∑(((y_pred .- y_true) .^ 2))

    r² = 1 - (ssr / sst)

    return r²
end


# Get the r-squared score for training and test datasets
train_r² = r_squared_score(train_preds, y_train)
test_r² = r_squared_score(test_preds, y_test)


println("Training R² score for test sets: ", train_r²)
println("Testing R² score for test sets: ", test_r²)
